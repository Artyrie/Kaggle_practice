{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85dae30e",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1b2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as pth\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import DataStructs\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')  \n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d162fb1",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92a30bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6925132695339830503\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10563764864\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8933212339793434640\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10563764864\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15613777631761335604\n",
      "physical_device_desc: \"device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:40:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10563764864\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 517385157965408797\n",
      "physical_device_desc: \"device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:3\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10563764864\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9534735448122488606\n",
      "physical_device_desc: \"device: 3, name: GeForce RTX 2080 Ti, pci bus id: 0000:42:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:4\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10563764864\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9084583867294831882\n",
      "physical_device_desc: \"device: 4, name: GeForce RTX 2080 Ti, pci bus id: 0000:60:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:5\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10563764864\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12185226337763909282\n",
      "physical_device_desc: \"device: 5, name: GeForce RTX 2080 Ti, pci bus id: 0000:62:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:6\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10563764864\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7411031491704544801\n",
      "physical_device_desc: \"device: 6, name: GeForce RTX 2080 Ti, pci bus id: 0000:63:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:7\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10563764864\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12651508463714054148\n",
      "physical_device_desc: \"device: 7, name: GeForce RTX 2080 Ti, pci bus id: 0000:64:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad1678d",
   "metadata": {},
   "source": [
    "### Multi Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d726169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    }
   ],
   "source": [
    "#tf.compat.v1.disable_eager_execution()\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices = [\"/gpu:4\", \"/gpu:5\", \"/gpu:6\", \"/gpu:7\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f685ce",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb4da95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../molecular_data/'\n",
    "TRAIN_DIR = PATH + 'train'\n",
    "TEST_DIR = PATH + 'test'\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "class CFG : \n",
    "    debug = False\n",
    "    size = 300\n",
    "    data_ut = 2424186 #500000\n",
    "    seed = 42\n",
    "    batch_size = 256\n",
    "    buffer_size = 100\n",
    "    learning_rate = 1e-4\n",
    "    base_channel = 8\n",
    "    model_encoder_name = 'CustomDenseNet-121'\n",
    "    model_base_path = pth.join('model', 'checkpoint')\n",
    "    \n",
    "    epochs = 30\n",
    "    n_mht = 512\n",
    "    n_layer = 4\n",
    "    n_dff = 1024\n",
    "    n_head = 8\n",
    "    dropout = 0.1 # 0\n",
    "    max_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571b119",
   "metadata": {},
   "source": [
    "## Label Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "114d09cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000011a64c74</td>\n",
       "      <td>InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000019cc0cd2</td>\n",
       "      <td>InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000252b6d2b</td>\n",
       "      <td>InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000026b49b7e</td>\n",
       "      <td>InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000026fc6c36</td>\n",
       "      <td>InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                              InChI\n",
       "0  000011a64c74  InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...\n",
       "1  000019cc0cd2  InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...\n",
       "2  0000252b6d2b  InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...\n",
       "3  000026b49b7e  InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...\n",
       "4  000026fc6c36  InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(PATH + 'train_labels.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "863d7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(img_name) : \n",
    "    return f\"{TRAIN_DIR}/{img_name[0]}/{img_name[1]}/{img_name[2]}/{img_name}.png\"\n",
    "\n",
    "train['path'] = train['image_id'].apply(get_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1434ad4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000011a64c74</td>\n",
       "      <td>InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...</td>\n",
       "      <td>../molecular_data/train/0/0/0/000011a64c74.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000019cc0cd2</td>\n",
       "      <td>InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...</td>\n",
       "      <td>../molecular_data/train/0/0/0/000019cc0cd2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000252b6d2b</td>\n",
       "      <td>InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...</td>\n",
       "      <td>../molecular_data/train/0/0/0/0000252b6d2b.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000026b49b7e</td>\n",
       "      <td>InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...</td>\n",
       "      <td>../molecular_data/train/0/0/0/000026b49b7e.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000026fc6c36</td>\n",
       "      <td>InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...</td>\n",
       "      <td>../molecular_data/train/0/0/0/000026fc6c36.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                              InChI  \\\n",
       "0  000011a64c74  InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...   \n",
       "1  000019cc0cd2  InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...   \n",
       "2  0000252b6d2b  InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...   \n",
       "3  000026b49b7e  InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...   \n",
       "4  000026fc6c36  InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...   \n",
       "\n",
       "                                             path  \n",
       "0  ../molecular_data/train/0/0/0/000011a64c74.png  \n",
       "1  ../molecular_data/train/0/0/0/000019cc0cd2.png  \n",
       "2  ../molecular_data/train/0/0/0/0000252b6d2b.png  \n",
       "3  ../molecular_data/train/0/0/0/000026b49b7e.png  \n",
       "4  ../molecular_data/train/0/0/0/000026fc6c36.png  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb60e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2424186"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['InChI'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ad926",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c0e35dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda280616c714a35920f9a02be3c59e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2424186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for idx in tqdm(train.index) : \n",
    "    caption = train['InChI'].iloc[idx]\n",
    "    img_name = train['path'].iloc[idx]\n",
    "    all_captions.append(caption)\n",
    "    all_img_name_vector.append(img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850c7e4",
   "metadata": {},
   "source": [
    "### Shuffle and Cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d5d729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2424186\n"
     ]
    }
   ],
   "source": [
    "train_captions, train_img_name_vector = shuffle(all_captions, all_img_name_vector, random_state = CFG.seed)\n",
    "train_captions = train_captions[:CFG.data_ut]\n",
    "train_img_name_vector = train_img_name_vector[:CFG.data_ut]\n",
    "print(len(train_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec538590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1939348, 484838, 1939348, 484838)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_name_train, img_name_val, caption_train, caption_val = train_test_split(train_img_name_vector, train_captions, test_size = 0.2, random_state = CFG.seed)\n",
    "len(img_name_train), len(img_name_val), len(caption_train), len(caption_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df4de04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_sgd = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f3c27",
   "metadata": {},
   "source": [
    "## Model setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76129b66",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f4898d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_base = CFG.model_encoder_name\n",
    "encoder_model_name = 'Autoencoder_{}_trts_basech_{:03d}'.format(encoder_model_base, CFG.base_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "986c1874",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_base_path = CFG.model_base_path\n",
    "encoder_model_path = pth.join(encoder_model_base_path, encoder_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c75afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(encoder_model_path, exist_ok = True)\n",
    "target_checkpoint_filename = sorted(os.listdir(encoder_model_path))[-1]\n",
    "encoder_model_filename = pth.join(encoder_model_path, target_checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53526844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/checkpoint/Autoencoder_CustomDenseNet-121_trts_basech_008/000015-0.001644-0.001633.hdf5'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45e8422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model_name = 'trfrm_mht_{}_layer_{}_dff_{}_head_{}_DO_{}'.format(\n",
    "        CFG.n_mht, CFG.n_layer, CFG.n_dff, CFG.n_head, CFG.dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0e876de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'enc-tr_{}_dec-tr_{}_len-100-all-pseudolabel'.format(encoder_model_name, decoder_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de28b00",
   "metadata": {},
   "source": [
    "### Max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "394ac482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "    \n",
    "# max_length = calc_max_length(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "648ea4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=False, char_level=True)\n",
    "# temp_captions = all_origin_captions + [\" ^#%()+-.0123456789=@ABCDEFGHIKLMNOPRSTVXYZ[\\\\]abcdefgilmnoprstuy$\"]\n",
    "# tokenizer.fit_on_texts(temp_captions)\n",
    "all_token_list = [\n",
    "    'c', 'C', '(', ')', '1', 'O', '=', '2', 'N', '<', '>', 'n', '[',\n",
    "    ']', '3', '@', 'H', 'l', 'S', '-', 'F', '+', '4', 's', 'o', '#',\n",
    "    'B', 'r', '.', '/', 'P', 'i', 'I', '5', '\\\\', 'e', 'A', 'a', 'g',\n",
    "    '6', 'u', 't', 'T', 'M', 'b', 'K', 'Z', '8', 'd', '9', 'R', 'G',\n",
    "    '7', 'L', 'V', 'h', 'W', 'p', 'm', 'E', 'Y', '0', 'U', 'f', 'D',\n",
    "    'y', 'k', 'X', ' ', '^', '%', '$'\n",
    "]\n",
    "tokenizer.fit_on_texts(all_token_list)\n",
    "top_k = len(tokenizer.word_index)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_seqs, maxlen = CFG.max_length, padding = 'post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f28a506a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InChI=1S/C21H22N6O4/c1-10-19-14(27-21(22)24-10)7-13(26-20(19)28)11-5-16(29-2)17(30-3)6-12(11)15-8-23-9-18(25-15)31-4/h5-6,8-9,13H,7H2,1-4H3,(H,26,28)(H2,22,24,27)/t13-/m1/s1'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc412d5d",
   "metadata": {},
   "source": [
    "### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94708572",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = CFG.batch_size\n",
    "BUFFER_SIZE = CFG.buffer_size\n",
    "d_model = CFG.n_mht\n",
    "num_layers = CFG.n_layer\n",
    "dff = CFG.n_dff\n",
    "num_heads = CFG.n_head\n",
    "vocab_size = top_k # + 1\n",
    "dropout_rate = CFG.dropout\n",
    "EPOCHS = CFG.epochs\n",
    "learning_rate = CFG.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4f48c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_steps = int(np.ceil(len(img_name_train) / BATCH_SIZE))\n",
    "val_num_steps = int(np.ceil(len(img_name_val) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37cf6f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d3bfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_path, caption) : \n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels = 1)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    img = img / 255.0\n",
    "    img = tf.image.resize(img, (CFG.size, CFG.size))\n",
    "    return img, caption\n",
    "\n",
    "def prep_func(img, caption) : \n",
    "    result_img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return result_img, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1dbd47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((img_name_train, caption_train))\n",
    "dataset_train = dataset_train.map(map_func, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "dataset_train = dataset_train.shuffle(CFG.buffer_size).batch(CFG.batch_size)\n",
    "dataset_train = dataset_train.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1864685",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = tf.data.Dataset.from_tensor_slices((img_name_val, caption_val))\n",
    "dataset_val = dataset_val.map(map_func, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "dataset_val = dataset_val.batch(CFG.batch_size)\n",
    "dataset_val = dataset_val.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf36b41",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33271d8e",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55b6b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "@tf.function\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "@tf.function\n",
    "def create_masks(tar):\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f20d6",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e555b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4bf22",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "149fd8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n",
    "\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        target_checkpoint_filename = sorted(os.listdir(encoder_model_path))[-1]\n",
    "        image_autoencoder = load_model(pth.join(encoder_model_path, target_checkpoint_filename))\n",
    "        image_features_extract_model = image_autoencoder.get_layer(encoder_model_base)\n",
    "#         image_features_extract_model.trainable = False\n",
    "        self.feature_extract_model = image_features_extract_model\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim, activation='relu')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.feature_extract_model(x)\n",
    "        x = tf.keras.layers.Reshape((-1, x.shape[3]))(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ImageCaptioningTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "               target_vocab_size, pe_target, rate=0.1):\n",
    "        super(ImageCaptioningTransformer, self).__init__()\n",
    "\n",
    "        self.encoder = CNN_Encoder(d_model)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ac5e1",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53295f5",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc4612c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "captioning_transformer = ImageCaptioningTransformer(\n",
    "    num_layers, d_model, num_heads, dff,\n",
    "    vocab_size, pe_target=100,\n",
    "    rate=dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a73827",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67b1207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=5):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9efa8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "if is_sgd == True:\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "# learning_rate = CustomSchedule(d_model)\n",
    "# optimizer = tf.keras.optimizers.Adam(\n",
    "#     learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "#     epsilon=1e-9\n",
    "# )\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none'\n",
    ")\n",
    "\n",
    "@tf.function\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccc87f4",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95a6e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = pth.join('model', 'decoder_checkpoint', model_name)\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    captioning_transformer = captioning_transformer, \n",
    "    optimizer = optimizer\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d1d2b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad92a6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1bd45eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(real, pred):\n",
    "#     pred = np.array(list(map(np.array, pred)))\n",
    "#     pred = np.moveaxis(pred, (0,1,2), (1,0,2))\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "#     print(real[:5], pred[:5])\n",
    "    real = real.numpy()\n",
    "    \n",
    "    score_list = []\n",
    "    for score_i, (each_pred, each_real) in enumerate(zip(pred, real)): \n",
    "        each_pred = ''.join([tokenizer.index_word.get(mol_i, '') for mol_i in each_pred])\n",
    "        each_pred = each_pred.split('>')[0]\n",
    "        m_pred = Chem.MolFromInchi(each_pred)\n",
    "        if m_pred == None:\n",
    "            score_list.append(0)\n",
    "            continue\n",
    "        each_real = ''.join([tokenizer.index_word.get(mol_i, '') for mol_i in each_real])\n",
    "        each_real = each_real[1:-1]\n",
    "        m_real = Chem.MolFromInchi(each_real)\n",
    "        \n",
    "        fp_pred = Chem.RDKFingerprint(m_pred)\n",
    "        fp_real = Chem.RDKFingerprint(m_real)\n",
    "        target_similarity = DataStructs.FingerprintSimilarity(fp_real,fp_pred)\n",
    "        score_list.append(target_similarity)\n",
    "        \n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "24b303e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function(input_signature=train_step_signature)\n",
    "@tf.function\n",
    "def train_step(img_tensor, target, training=True):\n",
    "    target_inp = target[:, :-1]\n",
    "    target_real = target[:, 1:]\n",
    "    \n",
    "    combined_mask = create_masks(target_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = captioning_transformer(\n",
    "            inp=img_tensor, tar=target_inp, training=training, \n",
    "            look_ahead_mask=combined_mask, dec_padding_mask=None\n",
    "#             look_ahead_mask=None, dec_padding_mask=None\n",
    "        )\n",
    "        loss = loss_function(target_real, predictions)\n",
    "#         total_loss = (loss / int(target_inp.shape[1]))\n",
    "        if training == True:\n",
    "            gradients = tape.gradient(loss, captioning_transformer.trainable_variables)    \n",
    "            optimizer.apply_gradients(zip(gradients, captioning_transformer.trainable_variables))\n",
    "\n",
    "    train_accuracy(target_real, predictions)\n",
    "    return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b6fddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "sim_plot, val_sim_plot = [], []\n",
    "lowest_val_loss = 1e12\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b5f16a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5f381fb4f74377930c9ee48d89f3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-6b05d11221a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mvalid_cap_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_cap_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_cap_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1217\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10445\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10446\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10447\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10448\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10449\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    total_loss, total_val_loss, total_test_pred_26_loss = 0, 0, 0\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    tqdm_dataset = tqdm(enumerate(dataset_train), total=train_num_steps, position=0, leave=True)\n",
    "    total_sim = 0\n",
    "    for (batch, (img_tensor, target)) in tqdm_dataset:\n",
    "        valid_cap_mask = (target[:,0] == 10)\n",
    "        img_tensor = img_tensor[valid_cap_mask]\n",
    "        target = target[valid_cap_mask]\n",
    "\n",
    "        batch_loss, pred_list = train_step(img_tensor, target, training=True)\n",
    "        smilarlity_list = calculate_similarity(target, pred_list)\n",
    "        smilarlity = np.mean(smilarlity_list)\n",
    "        total_sim += smilarlity\n",
    "        total_loss += batch_loss\n",
    "        if batch % 50 == 0:\n",
    "            tqdm_dataset.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Batch': batch,\n",
    "                'Loss': '{:06f}'.format(batch_loss.numpy() / int(target.shape[1])),\n",
    "                'Similarlity': smilarlity,\n",
    "                'Accuracy':train_accuracy.result().numpy(), \n",
    "            })\n",
    "        if batch % 30 == 0:\n",
    "            gc.collect()\n",
    "    loss_plot.append(total_loss / (batch+1))\n",
    "    sim_plot.append(total_sim / (batch+1))\n",
    "\n",
    "    tqdm_dataset_val = tqdm(enumerate(dataset_val), total=val_num_steps, position=0, leave=True)\n",
    "    total_val_sim = 0\n",
    "    for (batch, (img_tensor, target)) in tqdm_dataset_val:\n",
    "        valid_cap_mask = (target[:,0] == 10)\n",
    "        img_tensor = img_tensor[valid_cap_mask]\n",
    "        target = target[valid_cap_mask]\n",
    "\n",
    "        batch_val_loss, pred_list = train_step(img_tensor, target, training=False)\n",
    "        smilarlity_list = calculate_similarity(target, pred_list)\n",
    "        smilarlity = np.mean(smilarlity_list)\n",
    "        total_val_sim += smilarlity\n",
    "        total_val_loss += batch_val_loss\n",
    "        if batch % 50 == 0:\n",
    "            tqdm_dataset_val.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Batch': batch,\n",
    "                'Val Loss': '{:06f}'.format(batch_val_loss.numpy() / int(target.shape[1])),\n",
    "                'Var Similarlity': smilarlity,\n",
    "                'Var Accuracy':train_accuracy.result().numpy(), \n",
    "            })\n",
    "        if batch % 30 == 0:\n",
    "            gc.collect()\n",
    "    val_loss_plot.append(total_val_loss / (batch+1))\n",
    "    val_sim_plot.append(total_val_sim / (batch+1))\n",
    "\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    output.clear()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(loss_plot, label='loss')\n",
    "    plt.plot(val_loss_plot, label='val_loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Plot')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(sim_plot, label='similarity')\n",
    "    plt.plot(val_sim_plot, label='val_Similarity')\n",
    "    plt.ylim(-0.1,1.1)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Similarity')\n",
    "    plt.title('Similarity Plot')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    # print ('Epoch {}, Loss {:.6f}, Similiarity {:.6f}'.format(\n",
    "    #     epoch + 1, loss_plot[-1], sim_plot[-1]))    \n",
    "    print ('Epoch {}, Loss {:.6f}, Val loss: {:.6f}, Similiarity {:.6f}, Val similiarity: {:.6f}'.format(\n",
    "        epoch + 1, loss_plot[-1], val_loss_plot[-1], sim_plot[-1], val_sim_plot[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dba5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
